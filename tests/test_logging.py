import os
import sys
import time
from unittest import mock
from unittest.mock import patch

import numpy as np
import pytest

from common_utils.logging import LoggingHandler, init as logging_init, support_unobserve

# Test support_unobserve
@patch.dict(os.environ, {}, clear=True)
@patch('sys.argv', ['script_name', '--unobserve'])
def test_support_unobserve():
    support_unobserve()
    assert '--unobserve' not in sys.argv
    assert os.environ.get('WANDB_MODE') == 'dryrun'

# Test logging_init
@patch('common_utils.logging.wandb.init')
@patch.dict(os.environ, {'WANDB_ENTITY': 'test_entity', 'WANDB_PROJECT': 'test_project'}, clear=True)
@patch('sys.argv', ['script_name']) # Mock sys.argv for predictable tag
def test_logging_init_with_env_vars(mock_wandb_init):
    config = {'lr': 0.01}
    # The current implementation of logging_init asserts that env vars are present if args are not given,
    # but it doesn't actually USE the env vars for project/entity in the wandb.init call.
    # This test reflects that current (buggy) behavior: os.environ has the vars, so no assertion error from logging_init.
    # Then, wandb.init is called with project=None, entity=None.
    logging_init(config=config)
    mock_wandb_init.assert_called_once_with(
        project=None, # Bug: Not picked from os.environ['WANDB_PROJECT']
        entity=None,  # Bug: Not picked from os.environ['WANDB_ENTITY']
        config=config,
        tags=['script_name'], # Adjusted for auto-added script name
        notes=None
    )

@patch('common_utils.logging.wandb.init')
@patch('sys.argv', ['script_name']) # Mock sys.argv for predictable tag
def test_logging_init_with_args(mock_wandb_init):
    config = {'lr': 0.01}
    logging_init(config=config, project='arg_project', entity='arg_entity', tags=['tag1'], notes='test_note')
    mock_wandb_init.assert_called_once_with(
        project='arg_project',
        entity='arg_entity',
        config=config,
        tags=['tag1', 'script_name'], # Adjusted for auto-added script name
        notes='test_note'
    )

@patch.dict(os.environ, {}, clear=True)
@patch('common_utils.logging.wandb.init') # Mock wandb.init to prevent actual wandb calls
@patch('sys.argv', ['script_name']) # Mock sys.argv
def test_logging_init_missing_entity_project(mock_wandb_init):
    with pytest.raises(AssertionError, match='Please either pass in "entity" to logging.init or set environment variable \'WANDB_ENTITY\' to your wandb entity name.'):
        logging_init(config={'lr': 0.01})

    # Test with project provided but entity missing
    with pytest.raises(AssertionError, match='Please either pass in "entity" to logging.init or set environment variable \'WANDB_ENTITY\' to your wandb entity name.'):
        logging_init(config={'lr': 0.01}, project="test_project")

    # Test with entity provided but project missing
    with pytest.raises(AssertionError, match='Please either pass in "project" to logging.init or set environment variable \'WANDB_PROJECT\' to your wandb project name.'):
        logging_init(config={'lr': 0.01}, entity="test_entity")


# Tests for LoggingHandler
class TestLoggingHandler:

    def setup_method(self):
        self.handler = LoggingHandler()

    def test_logging_handler_log(self):
        self.handler.log({'metric1': 10, 'metric2': 20})
        assert 'metric1' in self.handler.log_dict
        assert 'metric2' in self.handler.log_dict
        assert self.handler.log_dict['metric1'] == [10]
        assert self.handler.log_dict['metric2'] == [20]

        # Ensure t_0 is set after first log
        assert self.handler.t_0 is not None
        first_t_0 = self.handler.t_0

        time.sleep(0.01) # Ensure time passes for between_log_time

        self.handler.log({'metric1': 15, 'metric2': 25})
        assert self.handler.log_dict['metric1'] == [10, 15]
        assert self.handler.log_dict['metric2'] == [20, 25]
        assert 'between_log_time' in self.handler.log_dict
        assert len(self.handler.log_dict['between_log_time']) == 1
        assert self.handler.log_dict['between_log_time'][0] > 0
        # self.handler.t_0 is updated with each log call in the current implementation
        assert self.handler.t_0 != first_t_0

    def test_logging_handler_reset(self):
        self.handler.log({'metric1': 10})
        assert 'metric1' in self.handler.log_dict
        assert self.handler.t_0 is not None

        self.handler.reset()
        assert self.handler.log_dict == {}
        assert self.handler.t_0 is None

    def test_logging_handler_flush(self):
        self.handler.log({'metric1': 10, 'metric2': 20})
        self.handler.log({'metric1': 20, 'metric2': 30}) # metric1 avg 15, metric2 avg 25

        # Mock time.time to control 'total_time'
        with patch('time.time', side_effect=[1.0, 2.5]): # t_0 will be 1.0, current time 2.5
            self.handler.t_0 = time.time() # Set t_0 for total_time calculation
            flushed_data = self.handler.flush()

        assert 'metric1' in flushed_data
        assert 'metric2' in flushed_data
        assert np.isclose(flushed_data['metric1'], 15)
        assert np.isclose(flushed_data['metric2'], 25)
        # 'total_time' is not implemented in the current LoggingHandler
        # assert 'total_time' in flushed_data
        # assert np.isclose(flushed_data['total_time'], 1.5)

        assert self.handler.log_dict == {} # Log dict should be cleared after flush
        assert self.handler.t_0 is None    # t_0 should be reset

    def test_logging_handler_flush_empty(self):
        # Test flushing when log_dict is empty
        flushed_data = self.handler.flush()
        assert flushed_data == {} # Should return an empty dict
        assert self.handler.log_dict == {}
        assert self.handler.t_0 is None

    def test_logging_handler_call(self):
        self.handler({'metric1': 5, 'metric2': np.array([1,2,3])}) # Test with simple value and numpy array
        assert 'metric1' in self.handler.log_dict
        assert self.handler.log_dict['metric1'] == [5]
        assert 'metric2' in self.handler.log_dict
        assert np.array_equal(self.handler.log_dict['metric2'][0], np.array([1,2,3]))

        self.handler({'metric1': 10})
        assert self.handler.log_dict['metric1'] == [5, 10]

    def test_logging_handler_log_single_value_metrics(self):
        self.handler.log({'sps': 100})
        self.handler.log({'sps': 110})
        flushed_data = self.handler.flush()
        assert 'sps' in flushed_data
        # Current implementation averages all metrics, no special handling for single_value_metrics
        assert np.isclose(flushed_data['sps'], 105)

    def test_logging_handler_log_avg_metrics(self):
        self.handler.log({'loss': 0.5}) # non single_value_metric
        self.handler.log({'loss': 0.3})
        flushed_data = self.handler.flush()
        assert 'loss' in flushed_data
        assert np.isclose(flushed_data['loss'], 0.4) # Should average

    def test_logging_handler_log_time_metrics(self):
        self.handler.log({'metric':1})
        time.sleep(0.01)
        self.handler.log({'metric':2})
        time.sleep(0.02)
        self.handler.log({'metric':3})

        flushed_data = self.handler.flush()
        assert 'between_log_time' in flushed_data
        # Current implementation averages between_log_time into a single float
        assert isinstance(flushed_data['between_log_time'], float)
        assert flushed_data['between_log_time'] > 0
        # 'total_time' is not implemented
        # assert 'total_time' in flushed_data
        # assert flushed_data['total_time'] > 0
        # The old assertion for total_time might not hold if between_log_time is averaged.
        # assert np.isclose(flushed_data['total_time'], flushed_data['between_log_time'][0] + flushed_data['between_log_time'][1])


    def test_logging_handler_with_numpy_arrays(self):
        self.handler.log({'np_metric': np.array([1, 2, 3])})
        self.handler.log({'np_metric': np.array([4, 5, 6])})
        flushed_data = self.handler.flush()
        assert 'np_metric' in flushed_data
        # Current np.mean behavior on a list of arrays might result in a single float (mean of all elements)
        # Expected: np.mean([np.array([1,2,3]), np.array([4,5,6])], axis=0) -> array([2.5, 3.5, 4.5])
        # Actual (based on logging.py): np.mean([array([1,2,3]), array([4,5,6])]) -> np.mean([[1,2,3],[4,5,6]]) -> 3.5
        assert np.isclose(flushed_data['np_metric'], 3.5)


    def test_logging_handler_with_mixed_types_error(self):
        self.handler.log({'mixed_metric': 10})
        # Current implementation does not raise TypeError for mixed types.
        # It will likely fail at np.mean() if types are incompatible for averaging.
        # For this test, we'll just ensure it doesn't raise TypeError on log
        try:
            self.handler.log({'mixed_metric': np.array([1,2,3])})
            # If the above doesn't raise, then try to flush to see if np.mean fails
            with pytest.raises(Exception): # Could be TypeError or ValueError from np.mean
                 self.handler.flush()
        except TypeError:
            pass # This is acceptable if log itself raises, though not current behavior


    def test_logging_handler_single_value_metrics_default(self):
        # Test that by default, metrics are averaged
        self.handler.log({'some_val': 10})
        self.handler.log({'some_val': 20})
        flushed_data = self.handler.flush()
        assert np.isclose(flushed_data['some_val'], 15)

    def test_logging_handler_custom_single_value_metrics(self):
        # Test with custom single_value_metrics list - this feature is not in current LoggingHandler
        # handler = LoggingHandler(single_value_metrics=['custom_sps'])
        handler = LoggingHandler() # No such argument
        handler.log({'custom_sps': 100})
        handler.log({'custom_sps': 110})
        handler.log({'loss': 0.5})
        handler.log({'loss': 0.3})
        flushed_data = handler.flush()
        # Current implementation averages all, as 'single_value_metrics' is not used
        assert np.isclose(flushed_data['custom_sps'], 105)
        assert np.isclose(flushed_data['loss'], 0.4)      # Averaged
